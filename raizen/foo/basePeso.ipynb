{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-datalake-store\n",
      "  Using cached azure_datalake_store-0.0.48-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from azure-datalake-store) (2.23.0)\n",
      "Collecting adal>=0.4.2\n",
      "  Downloading adal-1.2.3-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 763 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cffi in /opt/conda/lib/python3.7/site-packages (from azure-datalake-store) (1.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (1.25.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (2019.11.28)\n",
      "Requirement already satisfied: PyJWT>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure-datalake-store) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure-datalake-store) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure-datalake-store) (2.8.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi->azure-datalake-store) (2.20)\n",
      "Requirement already satisfied: six>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from cryptography>=1.1.0->adal>=0.4.2->azure-datalake-store) (1.14.0)\n",
      "Installing collected packages: adal, azure-datalake-store\n",
      "Successfully installed adal-1.2.3 azure-datalake-store-0.0.48\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-datalake-store \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.datalake.store import core, lib, multithread"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 1,
>>>>>>> dc432deb4672020172c48af038731a608c726d06
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 2,
>>>>>>> dc432deb4672020172c48af038731a608c726d06
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[8]\") \\\n",
    "            .appName(\"airflow_app\") \\\n",
    "            .config('spark.executor.memory', '16g') \\\n",
    "            .config('spark.driver.memory', '16g') \\\n",
    "            .config('spark.sql.execution.pandas.respectSessionTimeZone', False) \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"2048MB\") \\\n",
    "            .config(\"spark.port.maxRetries\", \"100\") \\\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code BZ5N36ZK2 to authenticate.\n"
     ]
    }
   ],
   "source": [
    "adlCreds = lib.auth(url_suffix='raizenprd01', resource='https://datalake.azure.net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.adl.oauth2.access.token.provider.type\", \"RefreshToken\")\n",
    "spark.conf.set(\"fs.adl.oauth2.client.id\", adlCreds.token['client'])\n",
    "spark.conf.set(\"fs.adl.oauth2.refresh.token\", adlCreds.token['refreshToken'])\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 4,
>>>>>>> dc432deb4672020172c48af038731a608c726d06
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType,StructField, StructType  \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.date_range(start='1-1-2012', end='31-12-2019').to_frame(index=False,name='Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
>>>>>>> dc432deb4672020172c48af038731a608c726d06
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               Date|\n",
      "+-------------------+\n",
      "|2012-01-01 00:00:00|\n",
      "|2012-01-02 00:00:00|\n",
      "|2012-01-03 00:00:00|\n",
      "|2012-01-04 00:00:00|\n",
      "|2012-01-05 00:00:00|\n",
      "|2012-01-06 00:00:00|\n",
      "|2012-01-07 00:00:00|\n",
      "|2012-01-08 00:00:00|\n",
      "|2012-01-09 00:00:00|\n",
      "|2012-01-10 00:00:00|\n",
      "|2012-01-11 00:00:00|\n",
      "|2012-01-12 00:00:00|\n",
      "|2012-01-13 00:00:00|\n",
      "|2012-01-14 00:00:00|\n",
      "|2012-01-15 00:00:00|\n",
      "|2012-01-16 00:00:00|\n",
      "|2012-01-17 00:00:00|\n",
      "|2012-01-18 00:00:00|\n",
      "|2012-01-19 00:00:00|\n",
      "|2012-01-20 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('dados/depara_feriados.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|                  _c0|\n",
      "+---------------------+\n",
      "| PK\u0003\u0004\u0014\u0000\u0006\u0000\b\u0000\u0000\u0000!\u0000�J�...|\n",
      "|  ��q\u0016J�\u0005d����h���\u0019U[|\n",
      "| ���y��R�ЈH�a���K�...|\n",
      "| �L{��\u001e",
      "o�\u0006�'\t���}�...|\n",
      "|           ��\u001b�?]X~8�|\n",
      "| %P�\u000f \u0014�\u0000�1E\u001a�Fh{�...|\n",
      "|Þco����R'\u001f�N7잗[Hb...|\n",
      "| ?�9L�ҙ\u0015�sbgٮ|�l!�...|\n",
      "| v-˷\u0002ݜ\u001c",
      "�<\u00130��O+IEC...|\n",
      "| T�\u0013.C�\u0004N�Uk\u0006s�*�`...|\n",
      "|                   �d|\n",
      "| -TM\u0000��l�������)c�...|\n",
      "| g\u0004\u0019\u001b9�e�Nb���&i�~...|\n",
      "| �\u0006X��-�}Z&�\u0002\u0000\u0000��\u0003...|\n",
      "| ��veck\u0005����#��hKl...|\n",
      "| ����ˏ4�\u000f?���C\u001a���...|\n",
      "| �<?~����\u000f���\u000f�P�...|\n",
      "| ���J��Y\u001d",
      "|�\u001b���$��...|\n",
      "| ����_/���sO�E�m̃\u001f...|\n",
      "|��t�矗0��S3��hw��1...|\n",
      "+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
