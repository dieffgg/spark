{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-datalake-store\n",
      "  Using cached azure_datalake_store-0.0.48-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from azure-datalake-store) (2.23.0)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.7/site-packages (from azure-datalake-store) (1.14.0)\n",
      "Collecting adal>=0.4.2\n",
      "  Using cached adal-1.2.3-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (2.9)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi->azure-datalake-store) (2.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure-datalake-store) (2.8.1)\n",
      "Requirement already satisfied: PyJWT>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure-datalake-store) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure-datalake-store) (2.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1.0->adal>=0.4.2->azure-datalake-store) (1.14.0)\n",
      "Installing collected packages: adal, azure-datalake-store\n",
      "Successfully installed adal-1.2.3 azure-datalake-store-0.0.48\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-datalake-store \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.datalake.store import core, lib, multithread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[8]\") \\\n",
    "            .appName(\"airflow_app\") \\\n",
    "            .config('spark.executor.memory', '16g') \\\n",
    "            .config('spark.driver.memory', '16g') \\\n",
    "            .config('spark.sql.execution.pandas.respectSessionTimeZone', False) \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"2048MB\") \\\n",
    "            .config(\"spark.port.maxRetries\", \"100\") \\\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code B5NX4JTDQ to authenticate.\n"
     ]
    }
   ],
   "source": [
    "adlCreds = lib.auth(url_suffix='raizenprd01', resource='https://datalake.azure.net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.adl.oauth2.access.token.provider.type\", \"RefreshToken\")\n",
    "spark.conf.set(\"fs.adl.oauth2.client.id\", adlCreds.token['client'])\n",
    "spark.conf.set(\"fs.adl.oauth2.refresh.token\", adlCreds.token['refreshToken'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType,StructField, StructType  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendario = pd.date_range(start='1-1-2012', end='01-01-2030').to_frame(index=False,name='Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(calendario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendario = spark_df.withColumn(\"Date\", F.to_date(F.col(\"Date\"), \"dd.MM.yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6576"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
