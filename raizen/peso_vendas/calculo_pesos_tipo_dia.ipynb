{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure.datalake.store\n",
      "  Using cached azure_datalake_store-0.0.48-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.7/site-packages (from azure.datalake.store) (1.14.0)\n",
      "Collecting adal>=0.4.2\n",
      "  Using cached adal-1.2.3-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from azure.datalake.store) (2.23.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi->azure.datalake.store) (2.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure.datalake.store) (2.8.1)\n",
      "Requirement already satisfied: PyJWT>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure.datalake.store) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure.datalake.store) (2.9.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure.datalake.store) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure.datalake.store) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure.datalake.store) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure.datalake.store) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1.0->adal>=0.4.2->azure.datalake.store) (1.14.0)\n",
      "Installing collected packages: adal, azure.datalake.store\n",
      "Successfully installed adal-1.2.3 azure.datalake.store\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyarrow\n",
    "# !pip install openpyxl\n",
    "!pip install azure.datalake.store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizando base de vendas pelos 12 meses anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizando_volume(df, n_dias=365):\n",
    "    '''\n",
    "    Normalizando os volumes pelos n_dias anteriores (default: 365 -> 12 meses)\n",
    "    '''\n",
    "    \n",
    "    #Agrupando os volumes dos terminais por dia\n",
    "    df_vendas_agg = df.sort_values(by=['Date']).set_index(['Date'], drop=True) \\\n",
    "                                               .groupby(by=['UF', 'Produto', 'Segmento']).resample('D').sum()\n",
    "    \n",
    "    #Excluindo Volumes negativos\n",
    "    df_vendas_agg = df_vendas_agg.query(\"Volume >= 0\")\n",
    "    \n",
    "    #Copiando DataFrame para posterior merge\n",
    "    df_vendas_agg_merge = df_vendas_agg.copy()\n",
    "    \n",
    "    #Resetando index\n",
    "    df_vendas_agg.reset_index(inplace=True)\n",
    "    df_vendas_agg.set_index(['Date'], drop=True, inplace=True)\n",
    "    \n",
    "    #Calculo da média dos n_dias anteriores\n",
    "    vol_medio = df_vendas_agg.groupby(by=['UF', 'Produto', 'Segmento']).rolling(n_dias).mean()\n",
    "    vol_medio.rename(columns={'Volume': 'vol_medio'}, inplace=True)\n",
    "    \n",
    "    #Merge nas bases para normalização\n",
    "    vendas_dia_final = df_vendas_agg_merge.merge(vol_medio, left_index=True, right_index=True)\n",
    "    \n",
    "    #Resetando index\n",
    "    vendas_dia_final.reset_index(inplace=True)\n",
    "    \n",
    "    #Normalizando pela média dos n_dias anteriores\n",
    "    vendas_dia_final['vol_norm'] = np.where(vendas_dia_final.vol_medio != 0.0,\n",
    "                                            vendas_dia_final.Volume / vendas_dia_final.vol_medio,\n",
    "                                            0.0)\n",
    "    \n",
    "    #Filtrando os produtos\n",
    "    vendas_dia_final = vendas_dia_final.query(\"Date >= '2012-04-01' and (Produto != 'Etanol' or Segmento != 'C') and \\\n",
    "                                                                        (Produto != 'Gasolina' or Segmento != 'C')\")\n",
    "    \n",
    "    return vendas_dia_final.drop(['vol_medio'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculando_pre_pos_feriado(df_dias_uteis):\n",
    "    '''\n",
    "    Calculando se os dias são feriados, pré feriado ou pós feriado\n",
    "    '''\n",
    "\n",
    "    #Criando colunas dia_util e feriado\n",
    "    df_dias_uteis['dia_util'] = np.where((df_dias_uteis.Nac == '0') & (df_dias_uteis.Est == '0') & (df_dias_uteis.Data.dt.weekday < 6), 1, 0)\n",
    "    df_dias_uteis['feriado'] = np.where((df_dias_uteis.Nac == '1') | (df_dias_uteis.Est == '1'), 1, 0)\n",
    "    \n",
    "    #Agrupando por Dia e UF para descobrir dias pós e pré feriado\n",
    "    calendario_uf = df_dias_uteis.groupby(['UF', 'Data']).agg({'feriado': sum})\n",
    "    calendario_uf['feriado'] = np.where(calendario_uf.feriado >= 1, 1, 0)\n",
    "    \n",
    "    #Shift para analisar se o dia posterior ou anterior foram feriados\n",
    "    calendario_uf['pos_feriado'] = calendario_uf.feriado.shift(1)\n",
    "    calendario_uf['pre_feriado'] = calendario_uf.feriado.shift(-1)\n",
    "    calendario_uf.reset_index(inplace=True)\n",
    "    \n",
    "    #Inserindo colunas de pré e pós feriado no DataFrame original\n",
    "    df_dias_uteis = df_dias_uteis.merge(calendario_uf, on=['UF', 'Data', 'feriado'])\n",
    "    dias_uteis_final = df_dias_uteis[['UF', 'Data', 'dia_util', 'feriado', 'pos_feriado', 'pre_feriado']]\n",
    "    \n",
    "    #Ajustando a base\n",
    "    dias_uteis_final.fillna(1, inplace=True) #2019-12-31 é pré-feriado e estava como NaN\n",
    "    dias_uteis_final.pos_feriado = dias_uteis_final.pos_feriado.astype(int)\n",
    "    dias_uteis_final.pre_feriado = dias_uteis_final.pre_feriado.astype(int)\n",
    "    \n",
    "    return dias_uteis_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculando_tipo_dia(df_dias_uteis, date_max):\n",
    "    '''\n",
    "    Calculando tipos de dia\n",
    "        - Dia Util\n",
    "        - Feriado\n",
    "        - Pré e pós feriado\n",
    "        - Dezembro\n",
    "    '''\n",
    "    \n",
    "    #Convertendo data_max em datetime\n",
    "    date_max = pd.to_datetime(date_max, format='%Y-%m-%d')\n",
    "    \n",
    "    #Filtrando para o período definido\n",
    "    df_dias_uteis = df_dias_uteis[(df_dias_uteis.Data <= date_max)]\n",
    "    \n",
    "    #Inserindo dia util, feriado, pré e pós feriado na base\n",
    "    dias_uteis_final = calculando_pre_pos_feriado(df_dias_uteis)\n",
    "    \n",
    "    # # # Definindo tipos de dias\n",
    "    dias_uteis_final['dia_tipo_semana'] = 0\n",
    "\n",
    "    # # Jan - Nov\n",
    "\n",
    "    #Semana normal\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 0) & (dias_uteis_final.dia_util == 1), 'segunda', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 1) & (dias_uteis_final.dia_util == 1), 'terca', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 2) & (dias_uteis_final.dia_util == 1), 'quarta', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 3) & (dias_uteis_final.dia_util == 1), 'quinta', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 4) & (dias_uteis_final.dia_util == 1), 'sexta', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 5) & (dias_uteis_final.dia_util == 1), 'sabado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 6) & (dias_uteis_final.dia_util == 0), 'domingo', dias_uteis_final.dia_tipo_semana)\n",
    "\n",
    "    #Feriado dia de semana\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 0) & (dias_uteis_final.feriado == 1), 'segunda_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 1) & (dias_uteis_final.feriado == 1), 'terca_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 2) & (dias_uteis_final.feriado == 1), 'quarta_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 3) & (dias_uteis_final.feriado == 1), 'quinta_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 4) & (dias_uteis_final.feriado == 1), 'sexta_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 5) & (dias_uteis_final.feriado == 1), 'sabado_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 6) & (dias_uteis_final.feriado == 1), 'domingo_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "\n",
    "    #Dia pré feriado\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 0) & (dias_uteis_final.pre_feriado == 1), 'segunda_pre_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 1) & (dias_uteis_final.pre_feriado == 1), 'terca_pre_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 2) & (dias_uteis_final.pre_feriado == 1), 'quarta_pre_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 3) & (dias_uteis_final.pre_feriado == 1), 'quinta_pre_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 4) & (dias_uteis_final.pre_feriado == 1), 'sexta_pre_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 5) & (dias_uteis_final.pre_feriado == 1), 'sabado_pre_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 6) & (dias_uteis_final.pre_feriado == 1), 'domingo_pre_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "\n",
    "    #Dia pós feriado\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 0) & (dias_uteis_final.pos_feriado == 1), 'segunda_pos_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 1) & (dias_uteis_final.pos_feriado == 1), 'terca_pos_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 2) & (dias_uteis_final.pos_feriado == 1), 'quarta_pos_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 3) & (dias_uteis_final.pos_feriado == 1), 'quinta_pos_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 4) & (dias_uteis_final.pos_feriado == 1), 'sexta_pos_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 5) & (dias_uteis_final.pos_feriado == 1), 'sabado_pos_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where((dias_uteis_final.Data.dt.weekday == 6) & (dias_uteis_final.pos_feriado == 1), 'domingo_pos_feriado', dias_uteis_final.dia_tipo_semana)\n",
    "\n",
    "    # # Dez\n",
    "\n",
    "    dias_uteis_final['dia_tipo_semana'] = np.where(dias_uteis_final.Data.dt.month == 12, dias_uteis_final.dia_tipo_semana + '_dez', dias_uteis_final.dia_tipo_semana)\n",
    "    \n",
    "    return dias_uteis_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_table(vendas_dia_norm, dias_uteis):\n",
    "    '''\n",
    "    Criação da master table\n",
    "        - Base dias úteis\n",
    "        - Base volume normalizado\n",
    "    '''\n",
    "    \n",
    "    #Merge nas bases\n",
    "    df_all = vendas_dia_norm.merge(dias_uteis, left_on=['Date', 'UF'], right_on=['Data', 'UF'], how='inner')\n",
    "    \n",
    "    #Dropando colunas que não interessam\n",
    "    df_all.drop(['Data', 'dia_util', 'feriado', 'pos_feriado', 'pre_feriado'], axis=1, inplace=True)\n",
    "    \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_vol_medio_dias_uteis(df):\n",
    "    '''\n",
    "    Calcula volume médio para dias úteis\n",
    "    '''\n",
    "    \n",
    "    #Criando lista de dias úteis\n",
    "    dia_util = ['segunda', 'terca', 'quarta', 'quinta', 'sexta', 'segunda_dez', \n",
    "                'terca_dez', 'quarta_dez', 'quinta_dez', 'sexta_dez']\n",
    "    \n",
    "    #Adicionando coluna dia_util\n",
    "    df['dia_util'] = np.where(df.dia_tipo_semana.isin(dia_util), 1, 0)\n",
    "\n",
    "    #Calculando Volume medio para dia util de cada UF/Produto/Segmento\n",
    "    df_dia_util = df.copy()\n",
    "    df_dia_util = df_dia_util.groupby(['UF', 'Produto', 'Segmento', 'dia_util']).mean().reset_index()\n",
    "    \n",
    "    #Filtrando apenas para dias úteis\n",
    "    df_dia_util = df_dia_util.query(\"dia_util == 1\")[['UF', 'Produto', 'Segmento', 'dia_util', 'vol_norm']]\n",
    "    \n",
    "    #Renomeando a coluna com o volume medio dos dias úteis\n",
    "    df_dia_util.rename(columns={'vol_norm': 'vol_medio_dia_util'}, inplace=True)\n",
    "    \n",
    "    return df_dia_util.drop(['dia_util'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao_pesos(df):\n",
    "    '''\n",
    "    Cálculo do erro de representação do dia frente a semana\n",
    "    '''\n",
    "    \n",
    "    #Criando colunas com o número da semana e ano\n",
    "    df['semana_num'] = df.Date.dt.weekofyear\n",
    "    df['ano'] = df.Date.dt.year\n",
    "    \n",
    "    #Soma semanal UF/Produto/Segmento\n",
    "    df_week = df.set_index(['Date']).groupby(['UF', 'Produto', 'Segmento']).resample('W').sum().reset_index()\n",
    "\n",
    "    #Criando coluna com numero da semana e ano para posterior Merge\n",
    "    df_week['semana_num'] = df_week.Date.dt.weekofyear\n",
    "    df_week['ano'] = df_week.Date.dt.year\n",
    "    \n",
    "    #Tratando o DataFrame\n",
    "    df_week.drop(['Date', 'vol_norm'], axis=1, inplace=True)\n",
    "    df_week.rename(columns={'Volume': 'vol_semanal', 'peso': 'peso_semanal'}, inplace=True)\n",
    "    \n",
    "    #Merge com a base completa\n",
    "    df = df.merge(df_week, on=['UF', 'Produto', 'Segmento', 'semana_num', 'ano'])\n",
    "    df.drop(['vol_norm'], axis=1, inplace=True)\n",
    "    \n",
    "    #Validação\n",
    "    df[\"Representatividade_Peso_Dia_na_Semana\"] = np.where(df.peso_semanal > 0,\n",
    "                                                           df.peso / df.peso_semanal,\n",
    "                                                           df.peso)\n",
    "\n",
    "    df[\"Representatividade_Volume_Dia_na_Semana\"] = np.where(df.vol_semanal > 0,\n",
    "                                                             df.Volume / df.vol_semanal,\n",
    "                                                             df.Volume)\n",
    "\n",
    "    df[\"Erro_Representatividade\"] = np.where(df.Representatividade_Volume_Dia_na_Semana > 0,\n",
    "                                             abs((df.Representatividade_Peso_Dia_na_Semana - df.Representatividade_Volume_Dia_na_Semana) / df.Representatividade_Volume_Dia_na_Semana),\n",
    "                                             df.Representatividade_Peso_Dia_na_Semana)\n",
    "\n",
    "    df[\"Erro_Representatividade\"] = np.where(df.Erro_Representatividade > 1,\n",
    "                                             1.0,\n",
    "                                             df.Erro_Representatividade)\n",
    "    \n",
    "    #Criando coluna Erro * Volume para realização da média ponderada\n",
    "    df['Erro_Ponderado'] = (df.Erro_Representatividade * df.Volume)\n",
    "\n",
    "    #Calculando erro ponderado\n",
    "    err_ponderado = df.Erro_Ponderado.sum() / df.Volume.sum()\n",
    "\n",
    "    #Calculando erro medio\n",
    "    err_medio = df.Erro_Representatividade.mean()\n",
    "\n",
    "    print(\"Erro medio: \",err_medio)\n",
    "    print(\"Erro ponderado: \",err_ponderado)\n",
    "    \n",
    "    return df[['UF', 'Produto', 'Segmento', 'Date', 'Volume', 'dia_tipo_semana', 'peso', 'Erro_Representatividade']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def reply_future(df, df_dias_uteis):\n",
    "    '''\n",
    "    Função para replicação dos pesos encontrados para as datas futuras\n",
    "    '''\n",
    "    \n",
    "    #Selecionando o período futuro no DataFrame df_dias_uteis\n",
    "    df_dias_uteis_future = df_dias_uteis[df_dias_uteis.Data >= df.Date.max()]\n",
    "    \n",
    "    #Criando pool para inserir UF, Produto e Segmento para cada dia da base df_dias_uteis_future\n",
    "    lista_uf = list(df_validado.UF.unique())\n",
    "    lista_prod = list(df_validado.Produto.unique())\n",
    "    lista_segm = list(df_validado.Segmento.unique())\n",
    "    pool = product(lista_uf, lista_prod, lista_segm) \n",
    "    \n",
    "    #Criando df vazio com as colunas de interesse\n",
    "    df_uf_prod_segm = pd.DataFrame(columns=['UF', 'Produto', 'Segmento'])\n",
    "\n",
    "    #Adicionando UF, Produto e Segmento ao df criado anteriormente\n",
    "    for uf, prod, segm in pool:\n",
    "        df_uf_prod_segm = df_uf_prod_segm.append({'UF': uf, 'Produto': prod, 'Segmento': segm}, ignore_index=True)\n",
    "\n",
    "    #Filtrando dados\n",
    "    df_uf_prod_segm = df_uf_prod_segm.query(\"(Produto != 'Etanol' or Segmento != 'C') and \\\n",
    "                                             (Produto != 'Gasolina' or Segmento != 'C')\")\n",
    "    \n",
    "    #Merge para criar DataFrame com datas futuras para cada UF, Produto e Segmento\n",
    "    df_future = df_dias_uteis_future.merge(df_uf_prod_segm, on=['UF'])\n",
    "    df_future.drop(['dia_util', 'feriado', 'pos_feriado', 'pre_feriado'], axis=1, inplace=True)\n",
    "    \n",
    "    #Encontrando o Erro de Representatividade médio para cada granularidade\n",
    "    df_err_medio = df.groupby(['UF', 'Produto', 'Segmento', 'dia_tipo_semana']) \\\n",
    "                     .agg({'Erro_Representatividade': np.mean}) \\\n",
    "                     .reset_index()\n",
    "    \n",
    "    #Dropando colunas que não interessam \n",
    "    df.drop(['Date', 'Volume', 'Erro_Representatividade'], axis=1, inplace=True)\n",
    "    \n",
    "    #Merge final\n",
    "    df_future_final = df.merge(df_err_medio, on=['UF', 'Produto', 'Segmento', 'dia_tipo_semana']).drop_duplicates()\n",
    "    df_future_final = df_future_final.merge(df_future, on=['UF', 'Produto', 'Segmento', 'dia_tipo_semana'])\n",
    "    \n",
    "    #Arrumando o DataFrame\n",
    "    df_future_final = df_future_final[['UF', 'Produto', 'Segmento', 'Data', 'dia_tipo_semana', 'peso', 'Erro_Representatividade']]\n",
    "    df_future_final.rename(columns={'Data': 'Date'}, inplace=True)\n",
    "    \n",
    "    return df_future_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45037)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45037)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45037)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45037)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45037)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45037)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45037)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45037)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45037)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/pyspark/sql/dataframe.py\", line 533, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o72.collectToPython\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:45037)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o72.collectToPython",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ca6405605c3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Limpando a base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/traceback_utils.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    981\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:45037)"
     ]
    }
   ],
   "source": [
    "#Abrindo base\n",
    "# df_vendas = pd.read_parquet('bases/vendas_dia.parquet')\n",
    "#df_vendas = pd.read_parquet('vendas_dia.parquet', engine='pyarrow')\n",
    "\n",
    "#Limpando a base\n",
    "a.toPandas()\n",
    "a = a.drop_duplicates()\n",
    "a = a.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizando os volumes\n",
    "vendas_dia_norm = normalizando_volume(df_vendas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo a base dias uteis e filtrando para UF\n",
    "#     df_dias_uteis = pd.read_parquet('bases/dias_uteis_chave.parquet')\n",
    "df_dias_uteis = pd.read_parquet('dias_uteis_chave.parquet')\n",
    "df_dias_uteis = df_dias_uteis[['UF', 'Data', 'Nac', 'Est']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando tipos de dias\n",
    "dias_uteis = calculando_tipo_dia(df_dias_uteis, date_max='2021-03-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Master table\n",
    "master = create_master_table(vendas_dia_norm, dias_uteis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando o volume médio para cada tipo de dia\n",
    "master_vol_tipo_dia = master.groupby(['UF', 'Produto', 'Segmento', 'dia_tipo_semana']) \\\n",
    "                            .agg({'vol_norm': np.mean}) \\\n",
    "                            .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando o volume médio de dias úteis\n",
    "master_vol_tipo_dia_aux = master_vol_tipo_dia.copy()\n",
    "master_vol_dias_uteis = calculo_vol_medio_dias_uteis(master_vol_tipo_dia_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge nas bases e cálculo do peso\n",
    "master_pesos = master_vol_tipo_dia.merge(master_vol_dias_uteis, on=['UF', 'Produto', 'Segmento'])\n",
    "\n",
    "master_pesos['peso'] = np.where(~master_pesos.vol_norm.isnull(), master_pesos.vol_norm / master_pesos.vol_medio_dia_util, 0)\n",
    "master_pesos.drop(['vol_norm', 'vol_medio_dia_util'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge com a master table original\n",
    "master = master.merge(master_pesos, on=['UF', 'Produto', 'Segmento', 'dia_tipo_semana'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro medio:  0.23286281713013263\n",
      "Erro ponderado:  0.1389134123765066\n"
     ]
    }
   ],
   "source": [
    "#Validando os pesos encontrados\n",
    "df_validado = validacao_pesos(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replicando os pesos para as datas futuras\n",
    "df_presente_final = df_validado.copy()\n",
    "df_future_final = reply_future(df_validado, dias_uteis)\n",
    "\n",
    "#Filtrando colunas de interesse\n",
    "df_presente_final.drop(['Volume'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenando os DataFrames com passado e futuro\n",
    "df_final = pd.concat([df_presente_final, df_future_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrando a base\n",
    "df_final = df_final.drop(['dia_tipo_semana'], axis=1).sort_values(by=['UF', 'Date'])\n",
    "df_final.rename(columns={'peso': 'Peso'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportando para Excel\n",
    "df_final.to_excel('racional_dias_uteis_peso.xlsx', index=False)\n",
    "\n",
    "#Exportando para parquet\n",
    "df_final.to_parquet('racional_dias_uteis_peso.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.datalake.store import core, lib, multithread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[8]\") \\\n",
    "            .appName(\"airflow_app\") \\\n",
    "            .config('spark.executor.memory', '16g') \\\n",
    "            .config('spark.driver.memory', '16g') \\\n",
    "            .config('spark.sql.execution.pandas.respectSessionTimeZone', False) \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"2048MB\") \\\n",
    "            .config(\"spark.port.maxRetries\", \"100\") \\\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code BSN6BZG8Z to authenticate.\n"
     ]
    }
   ],
   "source": [
    "adlCreds = lib.auth(url_suffix='raizenprd01', resource='https://datalake.azure.net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.adl.oauth2.access.token.provider.type\", \"RefreshToken\")\n",
    "spark.conf.set(\"fs.adl.oauth2.client.id\", adlCreds.token['client'])\n",
    "spark.conf.set(\"fs.adl.oauth2.refresh.token\", adlCreds.token['refreshToken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spark.read.parquet('adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/dieffgg/06_mastertable/master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
