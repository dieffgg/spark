{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-datalake-store in /opt/conda/lib/python3.7/site-packages (0.0.48)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from azure-datalake-store) (2.23.0)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.7/site-packages (from azure-datalake-store) (1.13.2)\n",
      "Requirement already satisfied: adal>=0.4.2 in /opt/conda/lib/python3.7/site-packages (from azure-datalake-store) (1.2.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (1.25.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure-datalake-store) (2019.11.28)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi->azure-datalake-store) (2.19)\n",
      "Requirement already satisfied: cryptography>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure-datalake-store) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure-datalake-store) (2.8.1)\n",
      "Requirement already satisfied: PyJWT>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure-datalake-store) (1.7.1)\n",
      "Requirement already satisfied: six>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from cryptography>=1.1.0->adal>=0.4.2->azure-datalake-store) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-datalake-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.datalake.store import core, lib, multithread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[8]\") \\\n",
    "            .appName(\"airflow_app\") \\\n",
    "            .config('spark.executor.memory', '16g') \\\n",
    "            .config('spark.driver.memory', '16g') \\\n",
    "            .config('spark.sql.execution.pandas.respectSessionTimeZone', False) \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"2048MB\") \\\n",
    "            .config(\"spark.port.maxRetries\", \"100\") \\\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code F7HL8QLDR to authenticate.\n"
     ]
    }
   ],
   "source": [
    "adlCreds = lib.auth(url_suffix='raizenprd01', resource='https://datalake.azure.net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.adl.oauth2.access.token.provider.type\", \"RefreshToken\")\n",
    "spark.conf.set(\"fs.adl.oauth2.client.id\", adlCreds.token['client'])\n",
    "spark.conf.set(\"fs.adl.oauth2.refresh.token\", adlCreds.token['refreshToken'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('sep','|').load('adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/01_raw/vendas_dia_complemento/mes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType,StructField, StructType  \n",
    "schema = StructType([\n",
    "        StructField('Empr', StringType(), True),\n",
    "        StructField('_c1', StringType(), True),\n",
    "        StructField('Cent', StringType(), True),\n",
    "        StructField('Nota Fiscal', StringType(), True),\n",
    "        StructField('Data Saida', StringType(), True),\n",
    "        StructField('Data Criacao', StringType(), True),\n",
    "        StructField('UF', StringType(), True),\n",
    "        StructField('Faturamento', StringType(), True),\n",
    "        StructField('Itm', StringType(), True),\n",
    "        StructField('Vencimento', StringType(), True),\n",
    "        StructField('TpOV', StringType(), True),\n",
    "        StructField('Ordem Vendas', StringType(), True),\n",
    "        StructField('IBM', StringType(), True),\n",
    "        StructField('Razao Social', StringType(), True),\n",
    "        StructField('Canal', StringType(), True),\n",
    "        StructField('_c15', StringType(), True),\n",
    "        StructField('Material', StringType(), True),\n",
    "        StructField('Descricao', StringType(), True),\n",
    "        StructField(' Quantidade', StringType(), True),\n",
    "        StructField('UM', StringType(), True),\n",
    "        StructField('Cond.Pgto', StringType(), True),\n",
    "        StructField('Inco', StringType(), True),\n",
    "        StructField('C.Exp', StringType(), True),\n",
    "        StructField('Valor item', StringType(), True),\n",
    "        StructField('Valor ZPFL', StringType(), True),\n",
    "        StructField('Valor ZPFN', StringType(), True),\n",
    "        StructField('Valor ZENC', StringType(), True),\n",
    "        StructField('Imposto', StringType(), True),\n",
    "        StructField('Valor ZDB1', StringType(), True),\n",
    "        StructField('Target', StringType(), True),\n",
    "        StructField('Block', StringType(), True),\n",
    "        StructField('UN', StringType(), True),\n",
    "        StructField('Valor ZDDB', StringType(), True),\n",
    "        StructField('Valor ZDIA', StringType(), True),\n",
    "        StructField('Valor ZDFR', StringType(), True),\n",
    "        StructField('Valor ICM3', StringType(), True),\n",
    "        StructField('Valor ICS3', StringType(), True),\n",
    "        StructField('Rep.Vendas|Acesso ZPFL', StringType(), True),\n",
    "        StructField('Volume', StringType(), True),\n",
    "        StructField('UN_', StringType(), True),\n",
    "        StructField('Hora Fat', StringType(), True),\n",
    "        StructField('ReferÃªncia', StringType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dia = spark.read.format('csv').option('sep',';').schema(schema).load('adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/01_raw/vendas_dia_complemento/dia').where(\"Empr is not null and Empr <> 'Empr' \").select('Data Saida','Cent','Material','IBM','Volume','Canal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+---------+--------+\n",
      "|Data Saida|Cent|Material|    IBM|   Volume|Segmento|\n",
      "+----------+----+--------+-------+---------+--------+\n",
      "|09.04.2020|BARC|22149801| 121410|10.000,00|       V|\n",
      "|09.04.2020|BARC|22149801|1032445| 5.000,00|       V|\n",
      "|09.04.2020|BARC|22150801|1032445| 5.000,00|       V|\n",
      "|09.04.2020|BARC|24314801|1032445| 5.000,00|       V|\n",
      "|09.04.2020|BARC|22149801|1046858|10.000,00|       V|\n",
      "+----------+----+--------+-------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dia = df_dia.withColumnRenamed('Canal','Segmento')\n",
    "df_dia.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+-------+--------+\n",
      "|Data Saida|Cent|Material|    IBM| Volume|Segmento|\n",
      "+----------+----+--------+-------+-------+--------+\n",
      "|09-04-2020|BARC|22149801| 121410|10000.0|       V|\n",
      "|09-04-2020|BARC|22149801|1032445| 5000.0|       V|\n",
      "|09-04-2020|BARC|22150801|1032445| 5000.0|       V|\n",
      "|09-04-2020|BARC|24314801|1032445| 5000.0|       V|\n",
      "|09-04-2020|BARC|22149801|1046858|10000.0|       V|\n",
      "|09-04-2020|BARC|24144801|1053586| 5000.0|       V|\n",
      "|09-04-2020|BARC|24314801|1053586|40000.0|       V|\n",
      "|09-04-2020|BARC|22149801|  33715| 5000.0|       V|\n",
      "|09-04-2020|BARC|22150801|  33715| 5000.0|       V|\n",
      "|09-04-2020|BARC|24144801|  33715| 5000.0|       V|\n",
      "|09-04-2020|BARC|22149801| 120570|10000.0|       V|\n",
      "|09-04-2020|BARC|22150801| 120570| 5000.0|       V|\n",
      "|09-04-2020|BARC|22149801|1033260| 5000.0|       V|\n",
      "|09-04-2020|BARC|24144801|1033260|25000.0|       V|\n",
      "|09-04-2020|BARC|24314801|1033260|25000.0|       V|\n",
      "|09-04-2020|BARC|22150801|1032436| 5000.0|       V|\n",
      "|09-04-2020|BARC|24144801|1032436| 7000.0|       V|\n",
      "|09-04-2020|BARC|24314801|1032436| 5000.0|       V|\n",
      "|09-04-2020|BARC|24144801|1033259|15000.0|       V|\n",
      "|09-04-2020|BARC|24314801|1033259|25000.0|       V|\n",
      "+----------+----+--------+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dia.withColumn('Volume', regexp_replace(\"Volume\", '\\.', ''))\\\n",
    "        .withColumn('Data Saida', regexp_replace(\"Data saida\", '\\.', '-'))\\\n",
    "        .withColumn('Volume', regexp_replace(\"Volume\", ',', '.').cast(\"double\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+---------+-----+\n",
      "|data_saida|Cent|Material|    IBM|   Volume|Canal|\n",
      "+----------+----+--------+-------+---------+-----+\n",
      "|09.04.2020|BARC|22149801| 121410|10.000,00|    V|\n",
      "|09.04.2020|BARC|22149801|1032445| 5.000,00|    V|\n",
      "|09.04.2020|BARC|22150801|1032445| 5.000,00|    V|\n",
      "|09.04.2020|BARC|24314801|1032445| 5.000,00|    V|\n",
      "|09.04.2020|BARC|22149801|1046858|10.000,00|    V|\n",
      "+----------+----+--------+-------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dia = df_dia.withColumnRenamed('Data Saida','data_saida').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+---------+--------+------------+\n",
      "|Cent|Material|    IBM|   Volume|Segmento|Data Tratada|\n",
      "+----+--------+-------+---------+--------+------------+\n",
      "|BARC|22149801| 121410|10.000,00|       V|  2020-04-09|\n",
      "|BARC|22149801|1032445| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|22150801|1032445| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|24314801|1032445| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|22149801|1046858|10.000,00|       V|  2020-04-09|\n",
      "|BARC|24144801|1053586| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|24314801|1053586|40.000,00|       V|  2020-04-09|\n",
      "|BARC|22149801|  33715| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|22150801|  33715| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|24144801|  33715| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|22149801| 120570|10.000,00|       V|  2020-04-09|\n",
      "|BARC|22150801| 120570| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|22149801|1033260| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|24144801|1033260|25.000,00|       V|  2020-04-09|\n",
      "|BARC|24314801|1033260|25.000,00|       V|  2020-04-09|\n",
      "|BARC|22150801|1032436| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|24144801|1032436| 7.000,00|       V|  2020-04-09|\n",
      "|BARC|24314801|1032436| 5.000,00|       V|  2020-04-09|\n",
      "|BARC|24144801|1033259|15.000,00|       V|  2020-04-09|\n",
      "|BARC|24314801|1033259|25.000,00|       V|  2020-04-09|\n",
      "+----+--------+-------+---------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dia.withColumn(\"Data Tratada\", F.to_date(F.col(\"Data Saida\"), \"dd.MM.yyyy\")).drop(\"Data Saida\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vendas_dia = spark.read.parquet('adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/03_primary/vendas_dia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'withColumn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-ae459c1a476c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_dia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data Saida'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data saida'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'withColumn'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+--------+-------+-------+--------+\n",
      "|               Date|Base|     SKU|    IBM| Volume|Segmento|\n",
      "+-------------------+----+--------+-------+-------+--------+\n",
      "|2018-05-23 00:00:00|APVE|15521801|1004717|  300.0|       A|\n",
      "|2018-05-23 00:00:00|APVE|15521801|1010270| 1929.0|       A|\n",
      "|2018-05-23 00:00:00|ARBR|15521801|1010270|  324.0|       A|\n",
      "|2018-05-23 00:00:00|ARBR|15521801|1014172|19949.0|       A|\n",
      "|2018-05-23 00:00:00|AREC|15509801|1011068| 1000.0|       A|\n",
      "+-------------------+----+--------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vendas_dia.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Mes`' given input columns: [Segmento, IBM, Volume, Base, Date, SKU];;\\n'Aggregate ['Mes, 'Ano, 'Produto, Segmento#140], ['Mes, 'Ano, 'Produto, Segmento#140, sum(Volume#139) AS sum(Volume)#172]\\n+- Relation[Date#135,Base#136,SKU#137,IBM#138,Volume#139,Segmento#140] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o124.agg.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Mes`' given input columns: [Segmento, IBM, Volume, Base, Date, SKU];;\n'Aggregate ['Mes, 'Ano, 'Produto, Segmento#140], ['Mes, 'Ano, 'Produto, Segmento#140, sum(Volume#139) AS sum(Volume)#172]\n+- Relation[Date#135,Base#136,SKU#137,IBM#138,Volume#139,Segmento#140] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:237)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:230)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:82)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:169)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-774fa22ca7b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvendas_dia_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_vendas_dia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Ano\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Produto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Segmento\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Volume\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exprs should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Mes`' given input columns: [Segmento, IBM, Volume, Base, Date, SKU];;\\n'Aggregate ['Mes, 'Ano, 'Produto, Segmento#140], ['Mes, 'Ano, 'Produto, Segmento#140, sum(Volume#139) AS sum(Volume)#172]\\n+- Relation[Date#135,Base#136,SKU#137,IBM#138,Volume#139,Segmento#140] parquet\\n\""
     ]
    }
   ],
   "source": [
    "vendas_dia_group = df_vendas_dia.groupBy(\"Mes\",\"Ano\",\"Produto\",\"Segmento\").agg({\"Volume\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vendas_dia_group' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-971fe564ff0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvendas_dia_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vendas_dia_group' is not defined"
     ]
    }
   ],
   "source": [
    "vendas_dia_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dia_group_pd = vendas_dia_group.orderBy(\"Date\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arguments should have the same length. The length of argument `y` is 2, whereas the length of previous arguments ['Date'] is 3225",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-acd324dcd0ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_dia_group_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sum(Volume)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/plotly/express/_chart_types.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(data_frame, x, y, line_group, color, line_dash, hover_name, hover_data, custom_data, text, facet_row, facet_col, facet_col_wrap, error_x, error_x_minus, error_y, error_y_minus, animation_frame, animation_group, category_orders, labels, color_discrete_sequence, color_discrete_map, line_dash_sequence, line_dash_map, log_x, log_y, range_x, range_y, line_shape, render_mode, title, template, width, height)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mpolyline\u001b[0m \u001b[0mmark\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0mD\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmake_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/plotly/express/_core.py\u001b[0m in \u001b[0;36mmake_figure\u001b[0;34m(args, constructor, trace_patch, layout_patch)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     args, trace_specs, grouped_mappings, sizeref, show_colorbar = infer_config(\n\u001b[0;32m-> 1368\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m     )\n\u001b[1;32m   1370\u001b[0m     \u001b[0mgrouper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mone_group\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped_mappings\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mone_group\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/plotly/express/_core.py\u001b[0m in \u001b[0;36minfer_config\u001b[0;34m(args, constructor, trace_patch)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mall_attrables\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgroup_attr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_attrables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_attrables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconstructor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreemap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSunburst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_dataframe_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/plotly/express/_core.py\u001b[0m in \u001b[0;36mbuild_dataframe\u001b[0;34m(args, attrables, array_attrables)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                         \u001b[0;34m\"The length of argument `%s` is %d, whereas the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m                         \u001b[0;34m\"length of previous arguments %s is %d\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m                         \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m                     )\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All arguments should have the same length. The length of argument `y` is 2, whereas the length of previous arguments ['Date'] is 3225"
     ]
    }
   ],
   "source": [
    "px.line(v_dia_group_pd, x=\"Date\", y=[\"meio\", \"sum(Volume)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
