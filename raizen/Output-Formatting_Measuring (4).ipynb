{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure.datalake.store\n",
      "  Using cached azure_datalake_store-0.0.48-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.7/site-packages (from azure.datalake.store) (1.14.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from azure.datalake.store) (2.23.0)\n",
      "Collecting adal>=0.4.2\n",
      "  Using cached adal-1.2.3-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi->azure.datalake.store) (2.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure.datalake.store) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure.datalake.store) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure.datalake.store) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->azure.datalake.store) (1.25.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure.datalake.store) (2.8.1)\n",
      "Requirement already satisfied: PyJWT>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure.datalake.store) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from adal>=0.4.2->azure.datalake.store) (2.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1.0->adal>=0.4.2->azure.datalake.store) (1.14.0)\n",
      "Installing collected packages: adal, azure.datalake.store\n",
      "Successfully installed adal-1.2.3 azure.datalake.store\n"
     ]
    }
   ],
   "source": [
    "!pip install azure.datalake.store \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code BD77FM7QT to authenticate.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as sum_\n",
    "from azure.datalake.store import core, lib, multithread\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[8]\") \\\n",
    "            .appName(\"airflow_app\") \\\n",
    "            .config('spark.executor.memory', '16g') \\\n",
    "            .config('spark.driver.memory', '16g') \\\n",
    "            .config('spark.sql.execution.pandas.respectSessionTimeZone', False) \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"2048MB\") \\\n",
    "            .config(\"spark.port.maxRetries\", \"100\") \\\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "adlCreds = lib.auth(url_suffix='raizenprd01', resource='https://datalake.azure.net/')\n",
    "\n",
    "spark.conf.set(\"fs.adl.oauth2.access.token.provider.type\", \"RefreshToken\")\n",
    "spark.conf.set(\"fs.adl.oauth2.client.id\", adlCreds.token['client'])\n",
    "spark.conf.set(\"fs.adl.oauth2.refresh.token\", adlCreds.token['refreshToken'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Preprocess: Lê os dados do lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/09_reporting/process_output/diesel_s10/backtest_diesels10.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o143.load.\n: org.apache.spark.sql.AnalysisException: Path does not exist: adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/09_reporting/process_output/diesel_s10/backtest_diesels10.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:558)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b25cff59ad01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m          'etanol/backtest_etanol.csv', 'gasolina/backtest_gasolina.csv']\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/09_reporting/process_output/diesel_s10/backtest_diesels10.csv;'"
     ]
    }
   ],
   "source": [
    "# Lê dados de predição\n",
    "df = pd.DataFrame()\n",
    "output_path = 'adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/09_reporting/process_output/'\n",
    "files = ['diesel_s10/backtest_diesels10.csv', 'diesel_s500/backtest_diesels500.csv', \n",
    "         'etanol/backtest_etanol.csv', 'gasolina/backtest_gasolina.csv']\n",
    "for f in files:\n",
    "    tmp = spark.read.format(\"csv\").option(\"header\", \"true\").load(output_path + f).toPandas()\n",
    "    df = pd.concat([df, tmp])\n",
    "    del tmp\n",
    "\n",
    "# Lê dados de realizado\n",
    "realizado = spark.read.format(\"csv\").option(\"header\", \"true\").load(output_path + './../realizado/realizado.csv').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Preprocess: substitui colunas de Cidade e UF por código IBGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Lê base de referência das cidades\n",
    "'''\n",
    "\n",
    "# Lê tabela com códigos de cidades e inclui na tabela principal, checa se não faltou nada, exclui colunas de cidade e uf\n",
    "cod_cidades = pd.read_csv('/home/jovyan/project/projecao-demanda/data/03_primary/cidade_ibge.txt', sep=';', encoding='latin1')\n",
    "cod_cidades = cod_cidades[['nome_cidade', 'sigla_uf', 'codigo_ibge']]\n",
    "cod_cidades.columns = ['Cidade', 'UF', 'cod_cidade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ajuste com base de predição\n",
    "'''\n",
    "\n",
    "# Inclui na tabela principal\n",
    "df = df.merge(cod_cidades, how='left')\n",
    "\n",
    "# Checa se não faltou nada\n",
    "cidades_sem_codigo = df[['Cidade', 'UF']].drop_duplicates()\n",
    "cidades_com_codigo = df[['Cidade', 'UF', 'cod_cidade']].drop_duplicates()\n",
    "(cidades_sem_codigo == cidades_com_codigo[['Cidade', 'UF']]).sum()/cidades_sem_codigo.shape[0]\n",
    "\n",
    "# exclui colunas de cidade e uf\n",
    "df = df.drop(['Base', 'Ano', 'Mes', 'Cidade', 'UF'], axis=1)\n",
    "\n",
    "# reformata tipos do dataframe\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "df.cutoff = pd.to_datetime(df.cutoff)\n",
    "df.Volume = df.Volume.astype('float32')\n",
    "\n",
    "# grava pickle pra facilitar se precisar restagar depois\n",
    "df.to_pickle('/home/jovyan/project/projecao-demanda/data/03_primary/backtest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'realizado' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-154cf48cb775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Inclui na tabela principal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrealizado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrealizado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcod_cidades\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mrealizado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrealizado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrealizado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrealizado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'realizado' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ajuste com base de realizado\n",
    "'''\n",
    "\n",
    "# Inclui na tabela principal\n",
    "realizado = realizado.merge(cod_cidades, how='left')\n",
    "realizado.Date = pd.to_datetime(realizado.Date)\n",
    "realizado.Mes = realizado.Mes.astype('int32')\n",
    "realizado.Ano = realizado.Ano.astype('int32')\n",
    "realizado.Volume = realizado.Volume.astype('float32')\n",
    "\n",
    "# Checa se não faltou nada\n",
    "cidades_sem_codigo_realizado = realizado[['Cidade', 'UF']].drop_duplicates()\n",
    "cidades_com_codigo_realizado = realizado[['Cidade', 'UF', 'cod_cidade']].drop_duplicates()\n",
    "(cidades_sem_codigo_realizado == cidades_com_codigo_realizado[['Cidade', 'UF']]).sum()/cidades_sem_codigo_realizado.shape[0]\n",
    "\n",
    "# exclui colunas de cidade e uf\n",
    "realizado = realizado.drop(['Cidade', 'UF'], axis=1)\n",
    "\n",
    "# grava pickle pra facilitar se precisar restagar depois\n",
    "realizado.to_pickle('/home/jovyan/project/projecao-demanda/data/03_primary/realizado.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Rateio Cidade x Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lê arquivos atualizados de predição (df) e realizado\n",
    "df = pd.read_pickle('/home/jovyan/project/projecao-demanda/data/03_primary/backtest.pkl')\n",
    "realizado = pd.read_pickle('/home/jovyan/project/projecao-demanda/data/03_primary/realizado.pkl')\n",
    "\n",
    "# Cria visualização com cutoff com lag de 28 dias, para que seja utilizado um rateio com dados históricos por base\n",
    "df['lag_cutoff'] = df.cutoff + timedelta(-28)\n",
    "df['Ano'] = df.lag_cutoff.dt.year\n",
    "df['Mes'] = df.lag_cutoff.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela com definição de rateio histórico\n",
    "realizado_rateio_total = realizado\\\n",
    "                                .groupby(['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto'])['Volume']\\\n",
    "                                .sum()\\\n",
    "                                .reset_index()\n",
    "\n",
    "realizado_rateio_base = realizado\\\n",
    "                                .groupby(['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto', 'Base'])['Volume']\\\n",
    "                                .sum()\\\n",
    "                                .reset_index()\n",
    "\n",
    "realizado_rateio_base = realizado_rateio_base.merge(\n",
    "    realizado_rateio_total, \n",
    "    on=['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto'],\n",
    "    suffixes=('_Base', '_Total'))\n",
    "realizado_rateio_base['Volume_Prop'] = realizado_rateio_base.Volume_Base / realizado_rateio_base.Volume_Total\n",
    "\n",
    "realizado_rateio_base.loc[realizado_rateio_base.Volume_Prop.isnull(),'Volume_Prop'] = 1.0\n",
    "\n",
    "# Inclui coluna com data do rateio\n",
    "realizado_rateio_base['Date_Rateio'] = pd.to_datetime(\n",
    "    realizado_rateio_base['Ano'].astype(str)+ realizado_rateio_base['Mes'].astype(str), \n",
    "    format='%Y%m')\n",
    "\n",
    "# Inclui descritivo da cidade\n",
    "realizado_rateio_base = realizado_rateio_base.merge(cod_cidades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.1) Primeira Tentativa: Rateio com histórico de 28 dias, chave Ano\\Mes\\cod_cidade\\Segmento\\Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devem ser preenchidas  187996  combinações de Ano/Mes/cod_cidade/Segmento/Produto.\n"
     ]
    }
   ],
   "source": [
    "pool = df[['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto']].drop_duplicates()\n",
    "print('Devem ser preenchidas ', pool.shape[0], ' combinações de Ano/Mes/cod_cidade/Segmento/Produto.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Realiza o preenchimento daquelas chaves (Ano/Mes/cod_cidade/Segmento/Produto) em que houve um realizado no mês\n",
    "anterior, e que podemos utilizar a distribuição das bases\n",
    "'''\n",
    "\n",
    "df1 = df.merge(\n",
    "    realizado_rateio_base[['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto', 'Base', 'Volume_Prop']], \n",
    "    how='left'\n",
    ")\n",
    "df1 = df1[~(df1.Base.isnull())]\n",
    "df1 = df1[['Date', 'cutoff', 'contexto', 'Segmento', 'Produto', 'Volume',\n",
    "       'cod_cidade', 'lag_cutoff', 'Ano', 'Mes', 'Base', 'Volume_Prop']]\n",
    "\n",
    "pool_df1 = df1\\\n",
    ".sort_values(['Date', 'cod_cidade', 'Segmento', 'Produto', 'Volume_Prop'])\\\n",
    ".drop_duplicates(['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ainda nulls:  74866\n",
      "Percentual:  0.39823187727398457\n"
     ]
    }
   ],
   "source": [
    "print('Ainda nulls: ', pool.shape[0] - pool_df1.shape[0])\n",
    "print('Percentual: ', (pool.shape[0] - pool_df1.shape[0]) / pool.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2) Segunda Tentativa: Incluir histórico mais recente preenchido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista a parte do DataFrame que falta realizar rateio\n",
    "pool_faltantes = pool[(pool.merge(pool_df1, how='outer', indicator=True)._merge=='left_only').values]\n",
    "df2 = df.merge(pool_faltantes, how='right')\n",
    "try:\n",
    "    df2 = df2.drop(['Base', 'Volume_Prop'], axis=1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Realiza o preenchimento daquelas chaves (Ano/Mes/cod_cidade/Segmento/Produto) em que não houve um realizado no mês\n",
    "anterior, então busca o último período em que houve um realizado no histórico da chave\n",
    "'''\n",
    "\n",
    "# Inclui informações de rateio de realizado com a defasagem de data específica\n",
    "df2 = df2.merge(\n",
    "    realizado_rateio_base[['Date_Rateio', 'cod_cidade', 'Segmento', 'Produto', 'Base', 'Volume_Prop']],\n",
    "    on = ['cod_cidade', 'Segmento', 'Produto'],\n",
    "    how='left'\n",
    ")\n",
    "df2['Rateio_Delta'] = (df2.Date - df2.Date_Rateio).dt.days\n",
    "\n",
    "# Mantém apenas linhas que possuem algum realizado posterior (e coloca o mais recente)\n",
    "df2.loc[(df2.Rateio_Delta<0.0), ['Date_Rateio', 'Base', 'Volume_Prop', 'Rateio_Delta']] = \\\n",
    "    [df2.iloc[0].Date_Rateio, df2.iloc[0].Base, df2.iloc[0].Volume_Prop, df2.iloc[0].Rateio_Delta]\n",
    "df2 = df2.merge(\n",
    "    df2[~(df2.Date_Rateio.isnull())].groupby(['cod_cidade', 'Produto', 'Segmento'])['Date_Rateio'].min().reset_index(),\n",
    "    how='right'\n",
    ")\n",
    "\n",
    "# Organiza base\n",
    "df2 = df2[~(df2.Base.isnull())]\n",
    "df2 = df2[['Date', 'cutoff', 'contexto', 'Segmento', 'Produto', 'Volume',\n",
    "       'cod_cidade', 'lag_cutoff', 'Ano', 'Mes', 'Base', 'Volume_Prop']]\n",
    "\n",
    "# Lista casos encontrados\n",
    "pool_df2 = df2\\\n",
    ".sort_values(['Date', 'cod_cidade', 'Segmento', 'Produto', 'Volume_Prop'])\\\n",
    ".drop_duplicates(['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto'], keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ainda nulls:  46958\n",
      "Percentual:  0.24978191025340965\n"
     ]
    }
   ],
   "source": [
    "print('Ainda nulls: ', pool.shape[0] - pool_df1.shape[0] - pool_df2.shape[0])\n",
    "print('Percentual: ', (pool.shape[0] - pool_df1.shape[0] - pool_df2.shape[0]) / pool.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3) Terceira tentativa: Lista a base que melhor atendeu aquele estado para o mesmo produto/segmento no ano-mes de referência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista a parte do DataFrame que falta realizar rateio\n",
    "pool_realizados = pd.concat([pool_df1, pool_df2], ignore_index=True)\n",
    "pool_faltantes = pool[(pool.merge(pool_realizados, how='outer', indicator=True)._merge=='left_only').values]\n",
    "df3 = df.merge(pool_faltantes, how='right')\n",
    "try:\n",
    "    df3 = df3.drop(['Base', 'Volume_Prop'], axis=1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Realiza o preenchimento daquelas chaves (Ano/Mes/cod_cidade/Segmento/Produto) em que não houve um realizado em nenhum\n",
    "ponto no histórico da chave, portanto \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria base com realizado por UF e Base\n",
    "realizado_UF_base = realizado_rateio_base\\\n",
    ".groupby(['Ano', 'Mes', 'UF', 'Produto', 'Segmento', 'Base'])['Volume_Base']\\\n",
    ".sum()\\\n",
    ".reset_index()\n",
    "\n",
    "# Cria base com realizado por UF\n",
    "realizado_UF = realizado_rateio_base\\\n",
    ".groupby(['Ano', 'Mes', 'UF', 'Produto', 'Segmento'])['Volume_Base']\\\n",
    ".sum()\\\n",
    ".reset_index()\n",
    "realizado_UF.columns = ['Ano', 'Mes', 'UF', 'Produto', 'Segmento', 'Volume_UF']\n",
    "\n",
    "# Define proporcionalidade de Base por UF\n",
    "realizado_UF_base = realizado_UF_base.merge(realizado_UF, how='left')\n",
    "realizado_UF_base['Volume_Prop'] = realizado_UF_base.Volume_Base / realizado_UF_base.Volume_UF\n",
    "\n",
    "# Inclui coluna com data do rateio\n",
    "realizado_UF_base['Date_Rateio'] = pd.to_datetime(\n",
    "    realizado_UF_base['Ano'].astype(str)+ realizado_UF_base['Mes'].astype(str), \n",
    "    format='%Y%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclui dados de proporcionalidade de Base por UF nos registros faltantes\n",
    "df3 = df3.merge(cod_cidades, how='left')\n",
    "df3 = df3.merge(\n",
    "    realizado_UF_base[['UF', 'Segmento', 'Produto', 'Base', 'Volume_Prop', 'Date_Rateio']], \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organiza base\n",
    "df3 = df3[~(df3.Base.isnull())]\n",
    "df3 = df3[['Date', 'cutoff', 'contexto', 'Segmento', 'Produto', 'Volume',\n",
    "       'cod_cidade', 'lag_cutoff', 'Ano', 'Mes', 'Base', 'Volume_Prop']]\n",
    "\n",
    "# Lista casos encontrados\n",
    "pool_df3 = df3\\\n",
    ".sort_values(['Date', 'cod_cidade', 'Segmento', 'Produto', 'Volume_Prop'])\\\n",
    ".drop_duplicates(['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date_Rateio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria base com realizado por UF e Base\n",
    "realizado_UF_base = realizado_rateio_base\\\n",
    ".groupby(['Ano', 'Mes', 'UF', 'Produto', 'Segmento', 'Base'])['Volume_Base']\\\n",
    ".sum()\\\n",
    ".reset_index()\n",
    "\n",
    "# Cria base com realizado por UF\n",
    "realizado_UF = realizado_rateio_base\\\n",
    ".groupby(['Ano', 'Mes', 'UF', 'Produto', 'Segmento'])['Volume_Base']\\\n",
    ".sum()\\\n",
    ".reset_index()\n",
    "realizado_UF.columns = ['Ano', 'Mes', 'UF', 'Produto', 'Segmento', 'Volume_UF']\n",
    "\n",
    "# Define proporcionalidade de Base por UF\n",
    "realizado_UF_base = realizado_UF_base.merge(realizado_UF, how='left')\n",
    "realizado_UF_base['Volume_Prop'] = realizado_UF_base.Volume_Base / realizado_UF_base.Volume_UF\n",
    "\n",
    "# Inclui dados de proporcionalidade de Base por UF nos registros faltantes\n",
    "df3 = df3.merge(cod_cidades, how='left')\n",
    "df3 = df3.merge(\n",
    "    realizado_UF_base[['Ano', 'Mes', 'UF', 'Segmento', 'Produto', 'Base', 'Volume_Prop']], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Organiza base\n",
    "df3 = df3[~(df3.Base.isnull())]\n",
    "df3 = df3[['Date', 'cutoff', 'contexto', 'Segmento', 'Produto', 'Volume',\n",
    "       'cod_cidade', 'lag_cutoff', 'Ano', 'Mes', 'Base', 'Volume_Prop']]\n",
    "\n",
    "# Lista casos encontrados\n",
    "pool_df3 = df3\\\n",
    ".sort_values(['Date', 'cod_cidade', 'Segmento', 'Produto', 'Volume_Prop'])\\\n",
    ".drop_duplicates(['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ainda nulls: ', pool.shape[0] - pool_df1.shape[0] - pool_df2.shape[0] - pool_df3.shape[0])\n",
    "print('Percentual: ', (pool.shape[0] - pool_df1.shape[0] - pool_df2.shape[0] - pool_df3.shape[0]) / pool.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_df2 = df2\\\n",
    ".sort_values(['Date', 'cod_cidade', 'Segmento', 'Produto', 'Volume_Prop'])\\\n",
    ".drop_duplicates(['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.cod_cidade==1100064) & (df.Date=='2019-04-28')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_df3.merge(cod_cidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pool_df3.Volume_Prop.isnull().sum()) / pool.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_df3.Volume_Prop.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[(~df3.Base.isnull()) & (df3.Rateio_Delta<0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realizado_rateio_base['Date_Rateio'] = pd.to_datetime(\n",
    "    realizado_rateio_base['Ano'].astype(str)+ realizado_rateio_base['Mes'].astype(str), \n",
    "    format='%Y%m')\n",
    "\n",
    "first = realizado_rateio_base.merge(\n",
    "    realizado_rateio_base.groupby(['cod_cidade', 'Produto', 'Segmento'])['Date_Rateio'].min().reset_index(),\n",
    "    how='right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(realizado_rateio_base[['Ano', 'Mes', 'Dia']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realizado_rateio_base[['cod_cidade', 'Produto', 'Segmento']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[~df1.Base.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\\\n",
    ".sort_values(['Date', 'cod_cidade', 'Segmento', 'Produto', 'Volume'])\\\n",
    ".drop_duplicates(['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto'], keep='last')\\\n",
    ".Volume.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realizado_rateio_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realizado_rateio_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realizado_rateio_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realizado_rateio_base.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    realizado_rateio_base[['Ano', 'Mes', 'cod_cidade', 'Segmento', 'Produto', 'Base', 'Volume_Prop']], \n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(unique_realizado, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.cod_cidade==4208906) & (df.Produto=='Diesel S10') & (df.Segmento=='B2B')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
