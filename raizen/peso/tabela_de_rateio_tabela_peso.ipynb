{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-datalake-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.datalake.store import core, lib, multithread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType,StructField, StructType  \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[8]\") \\\n",
    "            .appName(\"airflow_app\") \\\n",
    "            .config('spark.executor.memory', '16g') \\\n",
    "            .config('spark.driver.memory', '16g') \\\n",
    "            .config('spark.sql.execution.pandas.respectSessionTimeZone', False) \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"2048MB\") \\\n",
    "            .config(\"spark.port.maxRetries\", \"100\") \\\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code BKGYTJMRJ to authenticate.\n"
     ]
    }
   ],
   "source": [
    "adlCreds = lib.auth(url_suffix='raizenprd01', resource='https://datalake.azure.net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.adl.oauth2.access.token.provider.type\", \"RefreshToken\")\n",
    "spark.conf.set(\"fs.adl.oauth2.client.id\", adlCreds.token['client'])\n",
    "spark.conf.set(\"fs.adl.oauth2.refresh.token\", adlCreds.token['refreshToken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = spark.read.format('csv').option('header',True).option('sep',';').load('adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/static/TxConvGasolEq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = coef_df.select(\n",
    "        F.to_date(coef_df.inicio,\"dd/MM/yyyy\").alias('Inicio'),\n",
    "        F.to_date(coef_df.Fim,\"dd/MM/yyyy\").alias('Fim'),\n",
    "        F.regexp_replace(coef_df.TxConversao,\",\",\".\").cast(\"double\").alias('TxConversao')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_model = spark.read.parquet('adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/07_model_input/volume_historico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_historico = spark.read.parquet('adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/06_mastertable/master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------+--------+-----------+---------------+---+\n",
      "|               Date|Base| Volume|Segmento|    Produto|         Cidade| UF|\n",
      "+-------------------+----+-------+--------+-----------+---------------+---+\n",
      "|2012-06-28 00:00:00|BEST| 7000.0|       V|   Gasolina|   PORTO ALEGRE| RS|\n",
      "|2012-06-28 00:00:00|BEST| 7000.0|       V|   Gasolina|      CRUZ ALTA| RS|\n",
      "|2012-06-28 00:00:00|BEST| 3000.0|       V|   Gasolina|   CACHOEIRINHA| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|   Gasolina|   PORTO ALEGRE| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|   PORTO ALEGRE| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|  NOVO HAMBURGO| RS|\n",
      "|2012-06-28 00:00:00|BEST|31000.0|       V|Diesel S500|TRES CACHOEIRAS| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|         VIAMAO| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|   CACHOEIRINHA| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|   PORTO ALEGRE| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|   PORTO ALEGRE| RS|\n",
      "|2012-06-28 00:00:00|BEST|20000.0|       V|Diesel S500|         OSORIO| RS|\n",
      "|2012-06-28 00:00:00|BEST| 3000.0|       V|   Gasolina|      TRAMANDAI| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|   PORTO ALEGRE| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|          IVOTI| RS|\n",
      "|2012-06-28 00:00:00|BCUB| 5000.0|       V|   Gasolina|       REGISTRO| SP|\n",
      "|2012-06-28 00:00:00|BEST| 8000.0|       V|Diesel S500|  DOM FELICIANO| RS|\n",
      "|2012-06-28 00:00:00|BEST| 7000.0|       V|Diesel S500|   PORTO ALEGRE| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|   PORTO ALEGRE| RS|\n",
      "|2012-06-28 00:00:00|BEST| 5000.0|       V|Diesel S500|         CANOAS| RS|\n",
      "+-------------------+----+-------+--------+-----------+---------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "volume_historico.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_hist_uf = volume_historico \\\n",
    "                .groupBy('Date', 'Segmento', 'Produto', 'UF').agg({\"Volume\":\"sum\"}) \\\n",
    "                .withColumnRenamed('sum(Volume)','Volume_UF') \\\n",
    "                .withColumn('Date', F.to_date(volume_historico.Date,\"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Coef*Etanol) + Gasolina = Gasolina Equivalente\n",
    "df_gasolina_eq = vol_hist_uf.where(((F.col(\"Segmento\") == F.lit(\"V\")) & F.col(\"Produto\").isin({\"Etanol\", \"Gasolina\",})))\n",
    "df_gasolina_eq = df_gasolina_eq.groupBy(\"Date\",\"Segmento\", \"UF\").pivot(\"Produto\").sum(\"Volume_UF\")\n",
    "df_gasolina_eq = df_gasolina_eq.join(coef_df,[(F.col(\"Date\") > F.col(\"Inicio\")) & (F.col(\"Date\") < F.col(\"Fim\"))],\"inner\").select('Date','Segmento','Etanol','Gasolina','TxConversao')\n",
    "df_gasolina_eq = df_gasolina_eq.withColumn('GasolinaEq',F.col('Etanol')*F.col('TxConversao')+F.col('Gasolina'))\n",
    "df_gasolina_eq = df_gasolina_eq.selectExpr(\"Date\",\"Segmento\",\"stack(1, 'Gasolina', GasolinaEq) as (Produto, Volume_UF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diesel Equivalente - Varejo Soma Diesel S10 + Diesel S500\n",
    "df_s10 = vol_hist_uf.where(((F.col(\"Segmento\") == F.lit(\"V\")) & F.col(\"Produto\").isin({\"Diesel S10\", \"Diesel S500\"})))\n",
    "df_s10 = df_s10.groupBy(\"Date\",\"Segmento\").pivot(\"Produto\").sum(\"Volume_UF\") \\\n",
    ".withColumn('DieselEqVarejo',F.col('Diesel S10')+F.col('Diesel S500'))\n",
    "df_s10 = df_s10.selectExpr(\"Date\",\"Segmento\",\"stack(1, 'Diesel', DieselEqVarejo) as (Produto,Volume_UF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diesel Equivalente - B2B Soma Diesel S10 + Diesel S500\n",
    "df_b2b = vol_hist_uf.where(((F.col(\"Segmento\") == F.lit(\"C\")) & F.col(\"Produto\").isin({\"Diesel S10\", \"Diesel S500\"})))\n",
    "df_b2b = df_b2b.groupBy(\"Date\",\"Segmento\").pivot(\"Produto\").sum(\"Volume_UF\")\\\n",
    ".withColumn('DieselEqB2B',F.col('Diesel S10')+F.col('Diesel S500'))\n",
    "df_b2b = df_b2b.selectExpr(\"Date\",\"Segmento\",\"stack(1, 'Diesel', DieselEqB2B) as (Produto,Volume_UF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_uf = df_gasolina_eq.union(df_s10).union(df_b2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vol_hist_uf = volume_historico\\\n",
    "#                 .groupBy('Mes', 'Ano', 'UF', 'Segmento','Produto').agg({\"Volume\":\"sum\"}).withColumnRenamed('sum(Volume)','Volume_UF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_br = volume_historico.groupBy('Date', 'Segmento', 'Produto').agg({\"Volume\":\"sum\"}) \\\n",
    "        .withColumnRenamed('Date', 'YearMonth_Brasil') \\\n",
    "        .withColumnRenamed('Segmento', 'Segmento_Brasil') \\\n",
    "        .withColumnRenamed('Produto', 'Produto_Brasil') \\\n",
    "        .withColumnRenamed('sum(Volume)','Volume_Brasil')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_br = volume_br.withColumn('YearMonth_Brasil', F.to_date(volume_br.YearMonth_Brasil,\"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rateio = volume_uf.join(volume_br, (volume_uf.Date == volume_br.YearMonth_Brasil)  &   (volume_uf.Segmento == volume_br.Segmento_Brasil) & (volume_uf.Produto == volume_br.Produto_Brasil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rateio = rateio.withColumn('Rateio',F.col('Volume_UF')/F.col('Volume_Brasil')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o672.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 29.0 failed 1 times, most recent failure: Lost task 3.0 in stage 29.0 (TID 2269, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 212295 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:274)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0058a869579e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrateio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o672.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 29.0 failed 1 times, most recent failure: Lost task 3.0 in stage 29.0 (TID 2269, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 212295 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:274)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "rateio.show(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peso = spark.read.format('csv').option('header', True).option('sep',';').load('adl://raizenprd01.azuredatalakestore.net/ldt_dev/sandbox/previsao_demanda/static/peso_vendas_calendario.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peso = df_peso.groupBy('Date', 'UF', 'Produto', 'Segmento').agg({\"Peso\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+--------+-----------------+\n",
      "|     Date| UF|    Produto|Segmento|        sum(Peso)|\n",
      "+---------+---+-----------+--------+-----------------+\n",
      "| 3-Apr-12| MA|     Etanol|       V| 1.00429408716469|\n",
      "| 3-Apr-12| PI|   Gasolina|       V|0.883433977878056|\n",
      "| 3-Apr-12| RS|Diesel S500|       C| 0.97929933399968|\n",
      "| 4-Apr-12| MS|     Etanol|       V|0.820835511226811|\n",
      "| 5-Apr-12| CE| Diesel S10|       C|0.869423815634516|\n",
      "| 6-Apr-12| SC|Diesel S500|       C| 1.03348273093332|\n",
      "|10-Apr-12| RO|   Gasolina|       V|0.861782210598827|\n",
      "|16-Apr-12| RO|   Gasolina|       V| 1.02174061530452|\n",
      "|17-Apr-12| SP|Diesel S500|       V|0.933976224869288|\n",
      "|25-Apr-12| MG|     Etanol|       V|0.923999701327453|\n",
      "|26-Apr-12| PB|     Etanol|       V|0.914238222354483|\n",
      "|27-Apr-12| AL|   Gasolina|       V| 1.07002186387463|\n",
      "| 3-May-12| RJ|     Etanol|       V|0.866675327187963|\n",
      "| 3-May-12| RO|     Etanol|       V|0.735084985683681|\n",
      "|12-May-12| SC|     Etanol|       V|0.556862915183844|\n",
      "|12-May-12| SE|     Etanol|       V|0.790764219093153|\n",
      "|18-May-12| PI|     Etanol|       V|0.985815135185125|\n",
      "|19-May-12| PR|   Gasolina|       V|0.859426571418272|\n",
      "|19-May-12| SE|Diesel S500|       V|0.812976883621593|\n",
      "|19-May-12| SE|     Etanol|       V|0.790764219093153|\n",
      "+---------+---+-----------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_peso.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peso = df_peso.withColumn(\"Date\",(F.regexp_replace(df_peso.Date,\"-\",\"\"))) \\\n",
    "        .withColumn(\"Date\", F.to_date(F.col(\"Date\"), \"dd.MM.yyyy\"))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----------+--------+-----------------+\n",
      "|Date| UF|    Produto|Segmento|        sum(Peso)|\n",
      "+----+---+-----------+--------+-----------------+\n",
      "|null| MA|     Etanol|       V| 1.00429408716469|\n",
      "|null| PI|   Gasolina|       V|0.883433977878056|\n",
      "|null| RS|Diesel S500|       C| 0.97929933399968|\n",
      "+----+---+-----------+--------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_peso.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+--------+-----------------+----+----+----+\n",
      "|     Date| UF|    Produto|Segmento|        sum(Peso)| Ano| Mes| Dia|\n",
      "+---------+---+-----------+--------+-----------------+----+----+----+\n",
      "| 3-Apr-12| MA|     Etanol|       V| 1.00429408716469|null|null|null|\n",
      "| 3-Apr-12| PI|   Gasolina|       V|0.883433977878056|null|null|null|\n",
      "| 3-Apr-12| RS|Diesel S500|       C| 0.97929933399968|null|null|null|\n",
      "| 4-Apr-12| MS|     Etanol|       V|0.820835511226811|null|null|null|\n",
      "| 5-Apr-12| CE| Diesel S10|       C|0.869423815634516|null|null|null|\n",
      "| 6-Apr-12| SC|Diesel S500|       C| 1.03348273093332|null|null|null|\n",
      "|10-Apr-12| RO|   Gasolina|       V|0.861782210598827|null|null|null|\n",
      "|16-Apr-12| RO|   Gasolina|       V| 1.02174061530452|null|null|null|\n",
      "|17-Apr-12| SP|Diesel S500|       V|0.933976224869288|null|null|null|\n",
      "|25-Apr-12| MG|     Etanol|       V|0.923999701327453|null|null|null|\n",
      "|26-Apr-12| PB|     Etanol|       V|0.914238222354483|null|null|null|\n",
      "|27-Apr-12| AL|   Gasolina|       V| 1.07002186387463|null|null|null|\n",
      "| 3-May-12| RJ|     Etanol|       V|0.866675327187963|null|null|null|\n",
      "| 3-May-12| RO|     Etanol|       V|0.735084985683681|null|null|null|\n",
      "|12-May-12| SC|     Etanol|       V|0.556862915183844|null|null|null|\n",
      "|12-May-12| SE|     Etanol|       V|0.790764219093153|null|null|null|\n",
      "|18-May-12| PI|     Etanol|       V|0.985815135185125|null|null|null|\n",
      "|19-May-12| PR|   Gasolina|       V|0.859426571418272|null|null|null|\n",
      "|19-May-12| SE|Diesel S500|       V|0.812976883621593|null|null|null|\n",
      "|19-May-12| SE|     Etanol|       V|0.790764219093153|null|null|null|\n",
      "+---------+---+-----------+--------+-----------------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_peso = df_peso.withColumn(\"Dia\",F.date_format(F.col('Date'),\"dd\")) \\\n",
    "        .withColumn(\"Mes\",F.date_format(F.col('Date'),\"MM\")) \\\n",
    "        .withColumn(\"Ano\",F.date_format(F.col('Date'),\"YYYY\"))\n",
    "                \n",
    "df_peso.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------+--------+-----------------+\n",
      "|     Date| UF| Produto|Segmento|        sum(Peso)|\n",
      "+---------+---+--------+--------+-----------------+\n",
      "| 3-Apr-12| MA|  Etanol|       V| 1.00429408716469|\n",
      "| 3-Apr-12| PI|Gasolina|       V|0.883433977878056|\n",
      "| 4-Apr-12| MS|  Etanol|       V|0.820835511226811|\n",
      "|10-Apr-12| RO|Gasolina|       V|0.861782210598827|\n",
      "|16-Apr-12| RO|Gasolina|       V| 1.02174061530452|\n",
      "|25-Apr-12| MG|  Etanol|       V|0.923999701327453|\n",
      "|26-Apr-12| PB|  Etanol|       V|0.914238222354483|\n",
      "|27-Apr-12| AL|Gasolina|       V| 1.07002186387463|\n",
      "| 3-May-12| RJ|  Etanol|       V|0.866675327187963|\n",
      "| 3-May-12| RO|  Etanol|       V|0.735084985683681|\n",
      "|12-May-12| SC|  Etanol|       V|0.556862915183844|\n",
      "|12-May-12| SE|  Etanol|       V|0.790764219093153|\n",
      "|18-May-12| PI|  Etanol|       V|0.985815135185125|\n",
      "|19-May-12| PR|Gasolina|       V|0.859426571418272|\n",
      "|19-May-12| SE|  Etanol|       V|0.790764219093153|\n",
      "|21-May-12| DF|  Etanol|       V|0.837807055001337|\n",
      "|22-May-12| AL|Gasolina|       V|0.893293321301427|\n",
      "|27-May-12| BA|Gasolina|       V|0.162125998225635|\n",
      "| 7-Jun-12| PB|  Etanol|       V|0.914238222354483|\n",
      "|10-Jun-12| AL|Gasolina|       V|0.001043360657431|\n",
      "+---------+---+--------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_varejo = df_peso.where(((F.col(\"Segmento\") == F.lit(\"V\")) & F.col(\"Produto\").isin({\"Etanol\", \"Gasolina\",})))\n",
    "df_varejo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+--------+-----------------+\n",
      "|    Date| UF|    Produto|Segmento|             Peso|\n",
      "+--------+---+-----------+--------+-----------------+\n",
      "|1-Apr-12| AL|Diesel S500|       V|0.000793939173022|\n",
      "|1-Apr-12| BA|Diesel S500|       V|0.165102927097304|\n",
      "|1-Apr-12| CE| Diesel S10|       V|0.001777721194909|\n",
      "|1-Apr-12| ES|Diesel S500|       V| 0.03087335402366|\n",
      "|1-Apr-12| MA|Diesel S500|       V|0.005326310031586|\n",
      "|1-Apr-12| MG|Diesel S500|       V| 0.14445075784205|\n",
      "|1-Apr-12| PA| Diesel S10|       V|0.002458633759597|\n",
      "|1-Apr-12| PE| Diesel S10|       V|0.029104660549066|\n",
      "|1-Apr-12| PI|Diesel S500|       V|0.005386584525703|\n",
      "|1-Apr-12| PR|Diesel S500|       V|0.154093017400943|\n",
      "|1-Apr-12| RJ|Diesel S500|       V|0.109358808269283|\n",
      "|1-Apr-12| RS|Diesel S500|       V|0.002032309504466|\n",
      "|1-Apr-12| SE|Diesel S500|       V|0.073806739925784|\n",
      "|1-Apr-12| SP|Diesel S500|       V|0.082311557358312|\n",
      "|2-Apr-12| AL|Diesel S500|       V| 1.03098161415133|\n",
      "|2-Apr-12| BA|Diesel S500|       V| 1.05643654283543|\n",
      "|2-Apr-12| CE| Diesel S10|       V|0.952401832422428|\n",
      "|2-Apr-12| ES|Diesel S500|       V| 1.08728135234794|\n",
      "|2-Apr-12| MA|Diesel S500|       V| 1.03722444754584|\n",
      "|2-Apr-12| MG|Diesel S500|       V| 1.02355610969929|\n",
      "+--------+---+-----------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_varejo_diesel = df.where(((F.col(\"Segmento\") == F.lit(\"V\")) & F.col(\"Produto\").isin({\"Diesel S10\", \"Diesel S500\"})))\n",
    "df_varejo_diesel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+--------+-----------------+\n",
      "|    Date| UF|    Produto|Segmento|             Peso|\n",
      "+--------+---+-----------+--------+-----------------+\n",
      "|1-Apr-12| AL|Diesel S500|       C|                0|\n",
      "|1-Apr-12| BA|Diesel S500|       C|0.106229853387477|\n",
      "|1-Apr-12| CE| Diesel S10|       C|0.030886005500449|\n",
      "|1-Apr-12| ES|Diesel S500|       C|0.000179199636193|\n",
      "|1-Apr-12| MA|Diesel S500|       C|0.027962273990326|\n",
      "|1-Apr-12| MG| Diesel S10|       C|0.050711028668165|\n",
      "|1-Apr-12| MG|Diesel S500|       C|0.126477653119571|\n",
      "|1-Apr-12| PA| Diesel S10|       C|0.005766407356846|\n",
      "|1-Apr-12| PE| Diesel S10|       C|0.001318184493358|\n",
      "|1-Apr-12| PI|Diesel S500|       C|                0|\n",
      "|1-Apr-12| PR| Diesel S10|       C|0.103676433434961|\n",
      "|1-Apr-12| PR|Diesel S500|       C|0.205606122161058|\n",
      "|1-Apr-12| RJ| Diesel S10|       C|0.035321976717713|\n",
      "|1-Apr-12| RJ|Diesel S500|       C| 0.00844572208862|\n",
      "|1-Apr-12| RS| Diesel S10|       C|                0|\n",
      "|1-Apr-12| RS|Diesel S500|       C|0.001364820194306|\n",
      "|1-Apr-12| SC|Diesel S500|       C|0.004669525452018|\n",
      "|1-Apr-12| SP| Diesel S10|       C|0.014746802032761|\n",
      "|1-Apr-12| SP|Diesel S500|       C|0.087477639983275|\n",
      "|2-Apr-12| AL|Diesel S500|       C|0.728862783095246|\n",
      "+--------+---+-----------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b2b = df.where(((F.col(\"Segmento\") == F.lit(\"C\")) & F.col(\"Produto\").isin({\"Diesel S10\", \"Diesel S500\"})))\n",
    "df_b2b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, UF: string, Produto: string, Segmento: string, sum(Peso): double]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+--------+-----------------+\n",
      "|    Date| UF|    Produto|Segmento|             Peso|\n",
      "+--------+---+-----------+--------+-----------------+\n",
      "|1-Apr-12| AC|     Etanol|       V|                0|\n",
      "|1-Apr-12| AC|   Gasolina|       V|                0|\n",
      "|1-Apr-12| AL|Diesel S500|       C|                0|\n",
      "|1-Apr-12| AL|Diesel S500|       V|0.000793939173022|\n",
      "|1-Apr-12| AL|     Etanol|       V|0.001380148385852|\n",
      "|1-Apr-12| AL|   Gasolina|       V|0.001043360657431|\n",
      "|1-Apr-12| AM|     Etanol|       V|0.012257468537506|\n",
      "|1-Apr-12| AM|   Gasolina|       V|0.005977460471064|\n",
      "|1-Apr-12| BA|Diesel S500|       C|0.106229853387477|\n",
      "|1-Apr-12| BA|Diesel S500|       V|0.165102927097304|\n",
      "|1-Apr-12| BA|     Etanol|       V|0.145752385200027|\n",
      "|1-Apr-12| BA|   Gasolina|       V|0.162125998225635|\n",
      "|1-Apr-12| CE| Diesel S10|       C|0.030886005500449|\n",
      "|1-Apr-12| CE| Diesel S10|       V|0.001777721194909|\n",
      "|1-Apr-12| CE|     Etanol|       V|0.001916009334648|\n",
      "|1-Apr-12| CE|   Gasolina|       V|0.002980799554151|\n",
      "|1-Apr-12| DF|     Etanol|       V|0.014560315833427|\n",
      "|1-Apr-12| DF|   Gasolina|       V|0.029266554597407|\n",
      "|1-Apr-12| ES|Diesel S500|       C|0.000179199636193|\n",
      "|1-Apr-12| ES|Diesel S500|       V| 0.03087335402366|\n",
      "+--------+---+-----------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'cast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f96c3434b0fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dd.MM.yyyy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Peso\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Peso\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'double'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1300\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'cast'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'\"Peso\" is not a numeric column. Aggregation function can only be applied on a numeric column.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o780.sum.\n: org.apache.spark.sql.AnalysisException: \"Peso\" is not a numeric column. Aggregation function can only be applied on a numeric column.;\n\tat org.apache.spark.sql.RelationalGroupedDataset.$anonfun$aggregateNumericColumns$1(RelationalGroupedDataset.scala:101)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:237)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:230)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.RelationalGroupedDataset.aggregateNumericColumns(RelationalGroupedDataset.scala:98)\n\tat org.apache.spark.sql.RelationalGroupedDataset.sum(RelationalGroupedDataset.scala:296)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-9fe96a1b7f79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_varejo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Produto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Segmento'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Peso'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: '\"Peso\" is not a numeric column. Aggregation function can only be applied on a numeric column.;'"
     ]
    }
   ],
   "source": [
    "df_varejo = df.groupBy('Date', 'UF', 'Produto').pivot('Segmento').sum('Peso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"Peso\" among (Date, UF, Produto, Segmento, sum(Peso));'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o514.sum.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"Peso\" among (Date, UF, Produto, Segmento, sum(Peso));\n\tat org.apache.spark.sql.Dataset.$anonfun$resolve$1(Dataset.scala:223)\n\tat scala.Option.getOrElse(Option.scala:138)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:223)\n\tat org.apache.spark.sql.RelationalGroupedDataset.$anonfun$aggregateNumericColumns$1(RelationalGroupedDataset.scala:99)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:237)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:230)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.RelationalGroupedDataset.aggregateNumericColumns(RelationalGroupedDataset.scala:98)\n\tat org.apache.spark.sql.RelationalGroupedDataset.sum(RelationalGroupedDataset.scala:296)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-f34624856779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Produto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Segmento'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Peso'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"Peso\" among (Date, UF, Produto, Segmento, sum(Peso));'"
     ]
    }
   ],
   "source": [
    "df.groupBy('Date', 'Produto', 'Segmento').pivot('UF').sum('Peso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(((F.col(\"Segmento\") == F.lit(\"V\")) & F.col(\"Produto\").isin({\"Etanol\", \"Gasolina\",})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------+--------+-----------------+\n",
      "|    Date| UF| Produto|Segmento|             Peso|\n",
      "+--------+---+--------+--------+-----------------+\n",
      "|1-Apr-12| AC|  Etanol|       V|                0|\n",
      "|1-Apr-12| AC|Gasolina|       V|                0|\n",
      "|1-Apr-12| AL|  Etanol|       V|0.001380148385852|\n",
      "|1-Apr-12| AL|Gasolina|       V|0.001043360657431|\n",
      "|1-Apr-12| AM|  Etanol|       V|0.012257468537506|\n",
      "|1-Apr-12| AM|Gasolina|       V|0.005977460471064|\n",
      "|1-Apr-12| BA|  Etanol|       V|0.145752385200027|\n",
      "|1-Apr-12| BA|Gasolina|       V|0.162125998225635|\n",
      "|1-Apr-12| CE|  Etanol|       V|0.001916009334648|\n",
      "|1-Apr-12| CE|Gasolina|       V|0.002980799554151|\n",
      "+--------+---+--------+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
